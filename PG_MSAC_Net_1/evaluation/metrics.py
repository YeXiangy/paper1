"""
evaluation/metrics.py - è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
"""

import numpy as np
import torch
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve
)
from sklearn.preprocessing import label_binarize
from typing import Dict, List, Optional, Union, Tuple


def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray,
                      y_prob: Optional[np.ndarray] = None,
                      class_names: Optional[List[str]] = None) -> Dict[str, float]:
    """
    è®¡ç®—åˆ†ç±»æŒ‡æ ‡

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
        class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        metrics: æŒ‡æ ‡å­—å…¸
    """
    metrics = {}

    # åŸºç¡€æŒ‡æ ‡
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
    metrics['f1_score'] = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # åŠ æƒæŒ‡æ ‡
    metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
    metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)
    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)

    # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼ˆå¦‚æœæä¾›äº†ç±»åˆ«åç§°ï¼‰
    if class_names is not None:
        precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
        recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
        f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)

        for i, class_name in enumerate(class_names):
            if i < len(precision_per_class):
                metrics[f'precision_{class_name}'] = precision_per_class[i]
                metrics[f'recall_{class_name}'] = recall_per_class[i]
                metrics[f'f1_{class_name}'] = f1_per_class[i]

    # AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡ï¼‰
    if y_prob is not None:
        try:
            num_classes = len(np.unique(y_true))
            if num_classes == 2:
                # äºŒåˆ†ç±»
                metrics['auc'] = roc_auc_score(y_true, y_prob[:, 1])
            else:
                # å¤šåˆ†ç±»
                metrics['auc'] = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')
        except Exception as e:
            print(f"Warning: Could not compute AUC: {e}")
            metrics['auc'] = 0.0

    # æ··æ·†çŸ©é˜µ
    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)

    return metrics


def compute_class_metrics(y_true: np.ndarray, y_pred: np.ndarray,
                          class_names: List[str]) -> Dict[str, Dict[str, float]]:
    """
    è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        class_names: ç±»åˆ«åç§°åˆ—è¡¨

    Returns:
        class_metrics: æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡å­—å…¸
    """
    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)

    class_metrics = {}
    for i, class_name in enumerate(class_names):
        if i < len(precision_per_class):
            class_metrics[class_name] = {
                'precision': precision_per_class[i],
                'recall': recall_per_class[i],
                'f1_score': f1_per_class[i]
            }

    return class_metrics


def compute_confusion_matrix_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, Union[np.ndarray, float]]:
    """
    è®¡ç®—æ··æ·†çŸ©é˜µç›¸å…³æŒ‡æ ‡

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾

    Returns:
        metrics: æ··æ·†çŸ©é˜µæŒ‡æ ‡å­—å…¸
    """
    cm = confusion_matrix(y_true, y_pred)

    # æ€»ä½“å‡†ç¡®ç‡
    accuracy = np.trace(cm) / np.sum(cm)

    # æ¯ç±»å‡†ç¡®ç‡
    class_accuracy = np.diag(cm) / np.sum(cm, axis=1)

    # æ¯ç±»ç²¾ç¡®ç‡å’Œå¬å›ç‡
    precision_per_class = np.diag(cm) / np.sum(cm, axis=0)
    recall_per_class = np.diag(cm) / np.sum(cm, axis=1)

    return {
        'confusion_matrix': cm,
        'accuracy': accuracy,
        'class_accuracy': class_accuracy,
        'precision_per_class': precision_per_class,
        'recall_per_class': recall_per_class
    }


def compute_roc_metrics(y_true: np.ndarray, y_prob: np.ndarray,
                        class_names: Optional[List[str]] = None) -> Dict[str, Union[float, np.ndarray]]:
    """
    è®¡ç®—ROCç›¸å…³æŒ‡æ ‡

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡
        class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        roc_metrics: ROCæŒ‡æ ‡å­—å…¸
    """
    num_classes = len(np.unique(y_true))
    roc_metrics = {}

    if num_classes == 2:
        # äºŒåˆ†ç±»ROC
        fpr, tpr, thresholds = roc_curve(y_true, y_prob[:, 1])
        auc = roc_auc_score(y_true, y_prob[:, 1])

        roc_metrics['fpr'] = fpr
        roc_metrics['tpr'] = tpr
        roc_metrics['thresholds'] = thresholds
        roc_metrics['auc'] = auc

    else:
        # å¤šåˆ†ç±»ROC (one-vs-rest)
        y_true_binarized = label_binarize(y_true, classes=range(num_classes))

        fpr_dict = {}
        tpr_dict = {}
        auc_dict = {}

        for i in range(num_classes):
            fpr, tpr, _ = roc_curve(y_true_binarized[:, i], y_prob[:, i])
            auc = roc_auc_score(y_true_binarized[:, i], y_prob[:, i])

            class_name = class_names[i] if class_names and i < len(class_names) else f'Class_{i}'
            fpr_dict[class_name] = fpr
            tpr_dict[class_name] = tpr
            auc_dict[class_name] = auc

        roc_metrics['fpr_dict'] = fpr_dict
        roc_metrics['tpr_dict'] = tpr_dict
        roc_metrics['auc_dict'] = auc_dict
        roc_metrics['auc_macro'] = np.mean(list(auc_dict.values()))

    return roc_metrics


def compute_classification_report(y_true: np.ndarray, y_pred: np.ndarray,
                                  class_names: Optional[List[str]] = None) -> Dict:
    """
    ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        report: åˆ†ç±»æŠ¥å‘Šå­—å…¸
    """
    try:
        report = classification_report(
            y_true, y_pred,
            target_names=class_names,
            output_dict=True,
            zero_division=0
        )
        return report
    except Exception as e:
        print(f"Warning: Could not generate classification report: {e}")
        return {}


def compute_domain_adaptation_metrics(source_metrics: Dict[str, float],
                                      target_metrics: Dict[str, float]) -> Dict[str, float]:
    """
    è®¡ç®—åŸŸé€‚åº”ç›¸å…³æŒ‡æ ‡

    Args:
        source_metrics: æºåŸŸæŒ‡æ ‡
        target_metrics: ç›®æ ‡åŸŸæŒ‡æ ‡

    Returns:
        domain_metrics: åŸŸé€‚åº”æŒ‡æ ‡
    """
    domain_metrics = {}

    # æ€§èƒ½å·®è·
    domain_metrics['accuracy_gap'] = source_metrics['accuracy'] - target_metrics['accuracy']
    domain_metrics['f1_gap'] = source_metrics['f1_score'] - target_metrics['f1_score']

    # è¿ç§»ç‡
    domain_metrics['accuracy_transfer_ratio'] = target_metrics['accuracy'] / source_metrics['accuracy']
    domain_metrics['f1_transfer_ratio'] = target_metrics['f1_score'] / source_metrics['f1_score']

    # æ€§èƒ½ä¿æŒåº¦
    domain_metrics['accuracy_retention'] = 1 - abs(domain_metrics['accuracy_gap']) / source_metrics['accuracy']
    domain_metrics['f1_retention'] = 1 - abs(domain_metrics['f1_gap']) / source_metrics['f1_score']

    return domain_metrics


def evaluate_model_predictions(y_true: np.ndarray, y_pred: np.ndarray,
                               y_prob: Optional[np.ndarray] = None,
                               class_names: Optional[List[str]] = None) -> Dict:
    """
    ç»¼åˆè¯„ä¼°æ¨¡å‹é¢„æµ‹ç»“æœ

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        y_prob: é¢„æµ‹æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
        class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        evaluation_results: ç»¼åˆè¯„ä¼°ç»“æœ
    """
    results = {}

    # åŸºç¡€æŒ‡æ ‡
    results['basic_metrics'] = calculate_metrics(y_true, y_pred, y_prob, class_names)

    # æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    if class_names:
        results['class_metrics'] = compute_class_metrics(y_true, y_pred, class_names)

    # æ··æ·†çŸ©é˜µæŒ‡æ ‡
    results['confusion_metrics'] = compute_confusion_matrix_metrics(y_true, y_pred)

    # ROCæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰æ¦‚ç‡ï¼‰
    if y_prob is not None:
        results['roc_metrics'] = compute_roc_metrics(y_true, y_prob, class_names)

    # åˆ†ç±»æŠ¥å‘Š
    results['classification_report'] = compute_classification_report(y_true, y_pred, class_names)

    return results


def compare_model_performance(results1: Dict, results2: Dict,
                              model1_name: str = "Model 1",
                              model2_name: str = "Model 2") -> Dict:
    """
    æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½

    Args:
        results1: æ¨¡å‹1çš„è¯„ä¼°ç»“æœ
        results2: æ¨¡å‹2çš„è¯„ä¼°ç»“æœ
        model1_name: æ¨¡å‹1åç§°
        model2_name: æ¨¡å‹2åç§°

    Returns:
        comparison: æ¯”è¾ƒç»“æœ
    """
    comparison = {
        'models': [model1_name, model2_name],
        'metrics_comparison': {}
    }

    # æ¯”è¾ƒåŸºç¡€æŒ‡æ ‡
    for metric in ['accuracy', 'precision', 'recall', 'f1_score']:
        if metric in results1['basic_metrics'] and metric in results2['basic_metrics']:
            val1 = results1['basic_metrics'][metric]
            val2 = results2['basic_metrics'][metric]

            comparison['metrics_comparison'][metric] = {
                model1_name: val1,
                model2_name: val2,
                'difference': val2 - val1,
                'improvement': ((val2 - val1) / val1 * 100) if val1 > 0 else 0
            }

    return comparison


def compute_confidence_intervals(predictions: List[np.ndarray],
                                 targets: List[np.ndarray],
                                 confidence_level: float = 0.95) -> Dict[str, Tuple[float, float]]:
    """
    è®¡ç®—å¤šæ¬¡å®éªŒç»“æœçš„ç½®ä¿¡åŒºé—´

    Args:
        predictions: å¤šæ¬¡å®éªŒçš„é¢„æµ‹ç»“æœåˆ—è¡¨
        targets: å¤šæ¬¡å®éªŒçš„çœŸå®æ ‡ç­¾åˆ—è¡¨
        confidence_level: ç½®ä¿¡æ°´å¹³

    Returns:
        confidence_intervals: å„æŒ‡æ ‡çš„ç½®ä¿¡åŒºé—´
    """
    from scipy import stats

    metrics_list = []

    # è®¡ç®—æ¯æ¬¡å®éªŒçš„æŒ‡æ ‡
    for pred, target in zip(predictions, targets):
        metrics = calculate_metrics(target, pred)
        metrics_list.append(metrics)

    confidence_intervals = {}
    alpha = 1 - confidence_level

    # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„ç½®ä¿¡åŒºé—´
    for metric in ['accuracy', 'precision', 'recall', 'f1_score']:
        values = [m[metric] for m in metrics_list if metric in m]
        if values:
            mean_val = np.mean(values)
            std_val = np.std(values, ddof=1)
            n = len(values)

            # tåˆ†å¸ƒç½®ä¿¡åŒºé—´
            t_value = stats.t.ppf(1 - alpha / 2, n - 1)
            margin_error = t_value * std_val / np.sqrt(n)

            confidence_intervals[metric] = (
                mean_val - margin_error,
                mean_val + margin_error
            )

    return confidence_intervals


def compute_statistical_significance(results1: List[float],
                                     results2: List[float],
                                     test_type: str = 'ttest') -> Dict[str, float]:
    """
    è®¡ç®—ä¸¤ç»„ç»“æœçš„ç»Ÿè®¡æ˜¾è‘—æ€§

    Args:
        results1: ç¬¬ä¸€ç»„ç»“æœ
        results2: ç¬¬äºŒç»„ç»“æœ
        test_type: ç»Ÿè®¡æ£€éªŒç±»å‹ ('ttest', 'wilcoxon')

    Returns:
        significance: ç»Ÿè®¡æ˜¾è‘—æ€§ç»“æœ
    """
    from scipy import stats

    significance = {}

    if test_type == 'ttest':
        # é…å¯¹tæ£€éªŒ
        statistic, p_value = stats.ttest_rel(results1, results2)
        significance['test'] = 'Paired t-test'
    elif test_type == 'wilcoxon':
        # Wilcoxonç¬¦å·ç§©æ£€éªŒ
        statistic, p_value = stats.wilcoxon(results1, results2)
        significance['test'] = 'Wilcoxon signed-rank test'
    else:
        raise ValueError(f"Unknown test type: {test_type}")

    significance['statistic'] = statistic
    significance['p_value'] = p_value
    significance['significant'] = p_value < 0.05

    return significance


def format_metrics_table(metrics: Dict[str, float],
                         class_names: Optional[List[str]] = None) -> str:
    """
    æ ¼å¼åŒ–æŒ‡æ ‡ä¸ºè¡¨æ ¼å­—ç¬¦ä¸²

    Args:
        metrics: æŒ‡æ ‡å­—å…¸
        class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰

    Returns:
        table: æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    lines = []
    lines.append("=" * 60)
    lines.append("EVALUATION METRICS")
    lines.append("=" * 60)

    # æ€»ä½“æŒ‡æ ‡
    lines.append("Overall Metrics:")
    lines.append("-" * 30)
    for metric in ['accuracy', 'precision', 'recall', 'f1_score']:
        if metric in metrics:
            lines.append(f"  {metric.capitalize():12}: {metrics[metric]:.4f}")

    # åŠ æƒæŒ‡æ ‡
    lines.append("\nWeighted Metrics:")
    lines.append("-" * 30)
    for metric in ['precision_weighted', 'recall_weighted', 'f1_weighted']:
        if metric in metrics:
            name = metric.replace('_weighted', ' (weighted)')
            lines.append(f"  {name.capitalize():18}: {metrics[metric]:.4f}")

    # æ¯ç±»åˆ«æŒ‡æ ‡
    if class_names:
        lines.append("\nPer-Class Metrics:")
        lines.append("-" * 30)
        for class_name in class_names:
            lines.append(f"  {class_name}:")
            for metric in ['precision', 'recall', 'f1']:
                key = f"{metric}_{class_name}"
                if key in metrics:
                    lines.append(f"    {metric.capitalize():9}: {metrics[key]:.4f}")

    lines.append("=" * 60)

    return "\n".join(lines)


# ä¾¿æ·å‡½æ•°
def quick_evaluate(y_true: np.ndarray, y_pred: np.ndarray,
                   class_names: Optional[List[str]] = None,
                   print_results: bool = True) -> Dict[str, float]:
    """
    å¿«é€Ÿè¯„ä¼°å‡½æ•°

    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        class_names: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰
        print_results: æ˜¯å¦æ‰“å°ç»“æœ

    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡
    """
    metrics = calculate_metrics(y_true, y_pred, class_names=class_names)

    if print_results:
        print(format_metrics_table(metrics, class_names))

    return metrics


# æµ‹è¯•å‡½æ•°
if __name__ == '__main__':
    # æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—
    print("Testing evaluation metrics...")

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 200
    n_classes = 4

    y_true = np.random.randint(0, n_classes, n_samples)
    y_pred = np.random.randint(0, n_classes, n_samples)
    y_prob = np.random.rand(n_samples, n_classes)
    y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)  # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡

    class_names = ['Normal', 'Inner', 'Ball', 'Outer']

    # æµ‹è¯•åŸºç¡€æŒ‡æ ‡è®¡ç®—
    metrics = calculate_metrics(y_true, y_pred, y_prob, class_names)
    print("âœ… Basic metrics calculation successful")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"F1-Score: {metrics['f1_score']:.4f}")

    # æµ‹è¯•ç»¼åˆè¯„ä¼°
    results = evaluate_model_predictions(y_true, y_pred, y_prob, class_names)
    print("âœ… Comprehensive evaluation successful")

    # æµ‹è¯•å¿«é€Ÿè¯„ä¼°
    quick_metrics = quick_evaluate(y_true, y_pred, class_names, print_results=True)
    print("âœ… Quick evaluation successful")

    print("\nğŸ‰ All evaluation tests completed!")