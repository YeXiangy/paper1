#!/usr/bin/env python3
"""
PG-MSAC-Net ç®€å•æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰æ¨¡å—çš„åŸºæœ¬åŠŸèƒ½ï¼Œæ— éœ€çœŸå®žæ•°æ®é›†
"""

import torch
import torch.nn as nn
import numpy as np
from types import SimpleNamespace
import sys
import os
import traceback
from typing import Dict, Any, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))


def print_header(title: str):
    """æ‰“å°æµ‹è¯•æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"ðŸ” {title}")
    print("=" * 60)


def print_success(message: str):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"âœ… {message}")


def print_error(message: str):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"âŒ {message}")


def print_info(message: str):
    """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
    print(f"â„¹ï¸  {message}")


class TestResults:
    """æµ‹è¯•ç»“æžœç»Ÿè®¡"""

    def __init__(self):
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = []

    def add_test_result(self, test_name: str, passed: bool, error_msg: str = None):
        """æ·»åŠ æµ‹è¯•ç»“æžœ"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
            print_success(f"{test_name} passed")
        else:
            self.failed_tests.append((test_name, error_msg))
            print_error(f"{test_name} failed: {error_msg}")

    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 60)
        print("ðŸ“Š TEST SUMMARY")
        print("=" * 60)
        print(f"Total tests: {self.total_tests}")
        print(f"Passed: {self.passed_tests}")
        print(f"Failed: {len(self.failed_tests)}")

        if self.failed_tests:
            print("\nâŒ Failed tests:")
            for test_name, error in self.failed_tests:
                print(f"  â€¢ {test_name}: {error}")

        success_rate = (self.passed_tests / self.total_tests) * 100 if self.total_tests > 0 else 0
        print(f"\nSuccess rate: {success_rate:.1f}%")

        if len(self.failed_tests) == 0:
            print("\nðŸŽ‰ All tests passed! PG-MSAC-Net is ready to use.")
            return True
        else:
            print("\nâš ï¸  Some tests failed. Please check the issues above.")
            return False


def test_imports(results: TestResults):
    """æµ‹è¯•æ‰€æœ‰å…³é”®æ¨¡å—çš„å¯¼å…¥"""
    print_header("Testing Module Imports")

    # æµ‹è¯•é…ç½®æ¨¡å—
    try:
        from config import config
        results.add_test_result("Config import", True)
        print_info(f"Project: {config.project_name}")
        print_info(f"Device: {config.device}")
    except Exception as e:
        results.add_test_result("Config import", False, str(e))

    # æµ‹è¯•æ¨¡åž‹æ¨¡å—
    try:
        from models.PG_MSAC_Net import PG_MSAC_Net
        from models.MPIE import MultiLevelPhysicalEncoder
        from models.AMSCNN import AdaptiveMultiScaleCNN
        from models.MSGDA import MultiStatGuidedDomainAdapter
        results.add_test_result("Model modules import", True)
    except Exception as e:
        results.add_test_result("Model modules import", False, str(e))

    # æµ‹è¯•æ•°æ®é›†æ¨¡å—
    try:
        from datasets.JNU_bearing_dataset import JNUBearingDataset
        from datasets.data_utils import DataProcessor, BaseDataset
        results.add_test_result("Dataset modules import", True)
    except Exception as e:
        results.add_test_result("Dataset modules import", False, str(e))

    # æµ‹è¯•è®­ç»ƒæ¨¡å—
    try:
        from training.trainer import PGMSACTrainer
        from training.loss_functions import CrossDomainLoss
        results.add_test_result("Training modules import", True)
    except Exception as e:
        results.add_test_result("Training modules import", False, str(e))

    # æµ‹è¯•è¯„ä¼°æ¨¡å—
    try:
        from evaluation.evaluator import ModelEvaluator
        from evaluation.metrics import calculate_metrics
        results.add_test_result("Evaluation modules import", True)
    except Exception as e:
        results.add_test_result("Evaluation modules import", False, str(e))

    # æµ‹è¯•å·¥å…·æ¨¡å—
    try:
        from utils.checkpoint import CheckpointManager
        from utils.visualization import plot_training_curves
        results.add_test_result("Utility modules import", True)
    except Exception as e:
        results.add_test_result("Utility modules import", False, str(e))


def create_test_configs():
    """åˆ›å»ºæµ‹è¯•é…ç½®"""
    mpie_config = SimpleNamespace(
        time_features_dim=8, freq_features_dim=7, tf_features_dim=5,
        time_hidden_dim=48, freq_hidden_dim=40, tf_hidden_dim=40,
        output_dim=128
    )

    amscnn_config = SimpleNamespace(
        kernel_sizes=[3, 7, 15], scale_channels=64,
        conv_channels=[64, 128, 256, 512],
        physical_modulation_dim=192, dropout_rate=0.2
    )

    msgda_config = SimpleNamespace(
        num_statistical_features=6, weight_net_hidden=[32, 16],
        discriminator_hidden=[256, 128], discriminator_dropout=0.2,
        mmd_sigma=1.0
    )

    return mpie_config, amscnn_config, msgda_config


def test_individual_models(results: TestResults):
    """æµ‹è¯•å„ä¸ªæ¨¡åž‹ç»„ä»¶"""
    print_header("Testing Individual Model Components")

    device = torch.device('cpu')  # ä½¿ç”¨CPUé¿å…GPUä¾èµ–
    mpie_config, amscnn_config, msgda_config = create_test_configs()

    # æµ‹è¯•MPIE
    try:
        from models.MPIE import MultiLevelPhysicalEncoder

        mpie = MultiLevelPhysicalEncoder(mpie_config).to(device)
        test_input = torch.randn(4, 1, 1024)

        physical_code = mpie(test_input)
        expected_shape = (4, mpie_config.output_dim)

        assert physical_code.shape == expected_shape, f"Expected {expected_shape}, got {physical_code.shape}"
        results.add_test_result("MPIE forward pass", True)
        print_info(f"MPIE output shape: {physical_code.shape}")

        # æµ‹è¯•ç‰¹å¾é‡è¦æ€§
        importance = mpie.get_feature_importance(test_input)
        assert 'time_domain' in importance, "Missing time domain importance"
        results.add_test_result("MPIE feature importance", True)

    except Exception as e:
        results.add_test_result("MPIE forward pass", False, str(e))

    # æµ‹è¯•AMSCNN
    try:
        from models.AMSCNN import AdaptiveMultiScaleCNN

        amscnn = AdaptiveMultiScaleCNN(amscnn_config, physical_dim=128).to(device)
        test_signal = torch.randn(4, 1, 1024)
        test_physical = torch.randn(4, 128)

        deep_features = amscnn(test_signal, test_physical)
        expected_shape = (4, amscnn_config.conv_channels[-1])

        assert deep_features.shape == expected_shape, f"Expected {expected_shape}, got {deep_features.shape}"
        results.add_test_result("AMSCNN forward pass", True)
        print_info(f"AMSCNN output shape: {deep_features.shape}")

        # æµ‹è¯•æ³¨æ„åŠ›æƒé‡
        attention = amscnn.get_attention_weights(test_signal[:2], test_physical[:2])
        assert 'modulation_weights' in attention, "Missing modulation weights"
        results.add_test_result("AMSCNN attention weights", True)

    except Exception as e:
        results.add_test_result("AMSCNN forward pass", False, str(e))

    # æµ‹è¯•MSGDA
    try:
        from models.MSGDA import MultiStatGuidedDomainAdapter

        msgda = MultiStatGuidedDomainAdapter(msgda_config, feature_dim=512).to(device)
        source_features = torch.randn(4, 512)
        target_features = torch.randn(4, 512)

        domain_loss, adaptive_weight, loss_components = msgda(source_features, target_features)

        assert isinstance(domain_loss, torch.Tensor), "Domain loss should be a tensor"
        assert isinstance(adaptive_weight, torch.Tensor), "Adaptive weight should be a tensor"
        assert isinstance(loss_components, dict), "Loss components should be a dict"

        results.add_test_result("MSGDA forward pass", True)
        print_info(f"Domain loss: {domain_loss.item():.4f}")
        print_info(f"Adaptive weight: {adaptive_weight.item():.4f}")

    except Exception as e:
        results.add_test_result("MSGDA forward pass", False, str(e))


def test_complete_model(results: TestResults):
    """æµ‹è¯•å®Œæ•´çš„PG-MSAC-Netæ¨¡åž‹"""
    print_header("Testing Complete PG-MSAC-Net Model")

    device = torch.device('cpu')
    mpie_config, amscnn_config, msgda_config = create_test_configs()

    try:
        from models.PG_MSAC_Net import PG_MSAC_Net

        # åˆ›å»ºæ¨¡åž‹
        model = PG_MSAC_Net(
            num_classes=4, sample_len=1024,
            mpie_config=mpie_config, amscnn_config=amscnn_config,
            msgda_config=msgda_config
        ).to(device)

        results.add_test_result("Model creation", True)

        # æµ‹è¯•æ™®é€šå‰å‘ä¼ æ’­
        test_input = torch.randn(4, 1, 1024)
        output = model(test_input)
        expected_shape = (4, 4)

        assert output.shape == expected_shape, f"Expected {expected_shape}, got {output.shape}"
        results.add_test_result("Model forward pass", True)
        print_info(f"Model output shape: {output.shape}")

        # æµ‹è¯•ç‰¹å¾æå–
        features = model.extract_features(test_input)
        expected_features = ['physical_code', 'deep_features', 'classifier_features', 'raw_signal']

        for feature_name in expected_features:
            assert feature_name in features, f"Missing feature: {feature_name}"

        results.add_test_result("Feature extraction", True)
        print_info(f"Extracted features: {list(features.keys())}")

        # æµ‹è¯•æ¨¡åž‹å¤æ‚åº¦
        complexity = model.get_model_complexity()
        assert 'total_parameters' in complexity, "Missing parameter count"

        results.add_test_result("Model complexity analysis", True)
        print_info(f"Total parameters: {complexity['total_parameters']:,}")
        print_info(f"Model size: {complexity['model_size_mb']:.2f} MB")

        # æµ‹è¯•åŸŸé€‚åº”æ¨¡å¼
        target_features = model.extract_features(test_input)['deep_features']
        domain_output = model(test_input, target_features, domain_adapt=True)

        assert len(domain_output) == 4, "Domain adapt mode should return 4 outputs"
        results.add_test_result("Domain adaptation mode", True)

    except Exception as e:
        results.add_test_result("Complete model test", False, str(e))


def test_dataset_creation(results: TestResults):
    """æµ‹è¯•æ•°æ®é›†åˆ›å»ºï¼ˆä½¿ç”¨è™šæ‹Ÿæ•°æ®ï¼‰"""
    print_header("Testing Dataset Creation")

    try:
        from datasets.data_utils import BaseDataset
        from torch.utils.data import DataLoader

        # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†
        num_samples = 50
        sample_len = 1024
        num_classes = 4

        dummy_data = np.random.randn(num_samples, 1, sample_len).astype(np.float32)
        dummy_labels = np.random.randint(0, num_classes, num_samples)

        dataset = BaseDataset(dummy_data, dummy_labels)
        dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

        results.add_test_result("Dataset creation", True)
        print_info(f"Dataset size: {len(dataset)}")

        # æµ‹è¯•æ•°æ®åŠ è½½
        for batch_data, batch_labels in dataloader:
            assert batch_data.shape[1:] == (1, sample_len), f"Wrong data shape: {batch_data.shape}"
            assert batch_labels.shape[0] == batch_data.shape[0], "Batch size mismatch"
            break

        results.add_test_result("Data loading", True)
        print_info(f"Batch shape: {batch_data.shape}")

    except Exception as e:
        results.add_test_result("Dataset creation", False, str(e))


def test_training_components(results: TestResults):
    """æµ‹è¯•è®­ç»ƒç»„ä»¶"""
    print_header("Testing Training Components")

    try:
        from training.loss_functions import CrossDomainLoss
        from training.trainer import PGMSACTrainer
        from config import config

        # æµ‹è¯•æŸå¤±å‡½æ•°
        loss_fn = CrossDomainLoss(num_classes=4)

        # åˆ›å»ºè™šæ‹Ÿæ•°æ®
        logits = torch.randn(4, 4)
        targets = torch.randint(0, 4, (4,))
        outputs = {'logits': logits}

        total_loss, loss_components = loss_fn(outputs, targets)

        assert isinstance(total_loss, torch.Tensor), "Loss should be a tensor"
        assert isinstance(loss_components, dict), "Loss components should be a dict"

        results.add_test_result("Loss function", True)
        print_info(f"Total loss: {total_loss.item():.4f}")

        # æµ‹è¯•è®­ç»ƒå™¨åˆ›å»ºï¼ˆä¸è¿›è¡Œå®žé™…è®­ç»ƒï¼‰
        mpie_config, amscnn_config, msgda_config = create_test_configs()
        from models.PG_MSAC_Net import PG_MSAC_Net

        model = PG_MSAC_Net(
            num_classes=4, sample_len=1024,
            mpie_config=mpie_config, amscnn_config=amscnn_config,
            msgda_config=msgda_config
        )

        device = torch.device('cpu')
        trainer = PGMSACTrainer(model, config, device)

        results.add_test_result("Trainer creation", True)

        # æµ‹è¯•æ¨¡åž‹ä¿¡æ¯
        model_info = trainer.get_model_info()
        assert 'total_parameters' in model_info, "Missing model info"

        results.add_test_result("Model info extraction", True)
        print_info(f"Model: {model_info['model_name']}")
        print_info(f"Parameters: {model_info['total_parameters']:,}")

    except Exception as e:
        results.add_test_result("Training components", False, str(e))


def test_evaluation_components(results: TestResults):
    """æµ‹è¯•è¯„ä¼°ç»„ä»¶"""
    print_header("Testing Evaluation Components")

    try:
        from evaluation.metrics import calculate_metrics

        # åˆ›å»ºè™šæ‹Ÿé¢„æµ‹ç»“æžœ
        y_true = np.random.randint(0, 4, 100)
        y_pred = np.random.randint(0, 4, 100)
        y_prob = np.random.rand(100, 4)
        y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)  # å½’ä¸€åŒ–

        class_names = ['Normal', 'Inner', 'Ball', 'Outer']

        # è®¡ç®—æŒ‡æ ‡
        metrics = calculate_metrics(y_true, y_pred, y_prob, class_names)

        expected_metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        for metric in expected_metrics:
            assert metric in metrics, f"Missing metric: {metric}"

        results.add_test_result("Metrics calculation", True)
        print_info(f"Accuracy: {metrics['accuracy']:.4f}")
        print_info(f"F1-Score: {metrics['f1_score']:.4f}")

    except Exception as e:
        results.add_test_result("Evaluation components", False, str(e))


def test_utility_components(results: TestResults):
    """æµ‹è¯•å·¥å…·ç»„ä»¶"""
    print_header("Testing Utility Components")

    try:
        from utils.checkpoint import CheckpointManager
        import tempfile

        # æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        with tempfile.TemporaryDirectory() as temp_dir:
            manager = CheckpointManager(
                checkpoint_dir=temp_dir,
                max_checkpoints=3,
                monitor_metric='accuracy'
            )

            results.add_test_result("CheckpointManager creation", True)

            # æµ‹è¯•æœ€ä½³æ¨¡åž‹ä¿¡æ¯
            best_info = manager.get_best_model_info()
            assert 'best_metric' in best_info, "Missing best metric info"

            results.add_test_result("Checkpoint info", True)
            print_info(f"Checkpoint dir: {temp_dir}")

    except Exception as e:
        results.add_test_result("Utility components", False, str(e))


def test_configuration(results: TestResults):
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print_header("Testing Configuration System")

    try:
        from config import config

        # æµ‹è¯•é…ç½®è®¿é—®
        assert hasattr(config, 'data'), "Missing data config"
        assert hasattr(config, 'training'), "Missing training config"
        assert hasattr(config, 'mpie'), "Missing MPIE config"
        assert hasattr(config, 'amscnn'), "Missing AMSCNN config"
        assert hasattr(config, 'msgda'), "Missing MSGDA config"

        results.add_test_result("Config structure", True)

        # æµ‹è¯•é…ç½®æ›´æ–°
        original_batch_size = config.training.batch_size
        config.update_training_params(batch_size=32)
        assert config.training.batch_size == 32, "Config update failed"

        # æ¢å¤åŽŸå€¼
        config.training.batch_size = original_batch_size

        results.add_test_result("Config updates", True)

        # æµ‹è¯•å®žéªŒåç§°ç”Ÿæˆ
        exp_name = config.get_experiment_name()
        assert isinstance(exp_name, str) and len(exp_name) > 0, "Invalid experiment name"

        results.add_test_result("Experiment naming", True)
        print_info(f"Experiment name: {exp_name}")

    except Exception as e:
        results.add_test_result("Configuration system", False, str(e))


def test_memory_usage(results: TestResults):
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print_header("Testing Memory Usage")

    try:
        import psutil
        import gc

        # è®°å½•åˆå§‹å†…å­˜
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # åˆ›å»ºæ¨¡åž‹å¹¶è¿›è¡Œå‰å‘ä¼ æ’­
        mpie_config, amscnn_config, msgda_config = create_test_configs()
        from models.PG_MSAC_Net import PG_MSAC_Net

        model = PG_MSAC_Net(
            num_classes=4, sample_len=1024,
            mpie_config=mpie_config, amscnn_config=amscnn_config,
            msgda_config=msgda_config
        )

        # å¤šæ¬¡å‰å‘ä¼ æ’­æµ‹è¯•å†…å­˜æ³„æ¼
        for _ in range(10):
            test_input = torch.randn(8, 1, 1024)
            with torch.no_grad():
                output = model(test_input)
            del test_input, output

        gc.collect()
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory

        results.add_test_result("Memory usage test", True)
        print_info(f"Initial memory: {initial_memory:.1f} MB")
        print_info(f"Final memory: {final_memory:.1f} MB")
        print_info(f"Memory increase: {memory_increase:.1f} MB")

        if memory_increase > 500:  # è­¦å‘Šå¦‚æžœå†…å­˜å¢žé•¿è¶…è¿‡500MB
            print("âš ï¸  High memory usage detected")

    except ImportError:
        results.add_test_result("Memory usage test", False, "psutil not available")
    except Exception as e:
        results.add_test_result("Memory usage test", False, str(e))


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸš€ PG-MSAC-Net Comprehensive Test Suite")
    print("Testing all components without requiring real data...")

    results = TestResults()

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_functions = [
        test_imports,
        test_configuration,
        test_individual_models,
        test_complete_model,
        test_dataset_creation,
        test_training_components,
        test_evaluation_components,
        test_utility_components,
        test_memory_usage
    ]

    for test_func in test_functions:
        try:
            test_func(results)
        except Exception as e:
            results.add_test_result(f"{test_func.__name__}", False, f"Test crashed: {str(e)}")
            print_error(f"Test {test_func.__name__} crashed:")
            traceback.print_exc()

    # æ‰“å°æ€»ç»“
    success = results.print_summary()

    if success:
        print("\nðŸŽ¯ Next Steps:")
        print("1. Install dependencies: pip install -r requirements.txt")
        print("2. Prepare JNU dataset in: ./data/JNU/")
        print("3. Run training: python main.py --experiment_type cross_domain")
        print("\nðŸ“š For help:")
        print("â€¢ Quick test: python main.py --experiment_type quick_test")
        print("â€¢ Single run: python main.py --experiment_type single")
        return 0
    else:
        print("\nðŸ”§ Troubleshooting:")
        print("â€¢ Check Python version (>=3.8 recommended)")
        print("â€¢ Install missing dependencies")
        print("â€¢ Check file permissions")
        return 1


if __name__ == '__main__':
    exit(main())