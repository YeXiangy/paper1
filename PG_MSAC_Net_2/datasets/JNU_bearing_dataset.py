"""
JNU_bearing_dataset.py
JNUè½´æ‰¿æ•°æ®é›†å¤„ç†æ¨¡å— - ä¼˜åŒ–ç‰ˆ

ä¼˜åŒ–å†…å®¹:
1. æ”¯æŒæ‡’åŠ è½½æ¨¡å¼ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
2. æ”¹è¿›é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
3. æ”¯æŒæ•°æ®ç¼“å­˜æœºåˆ¶
4. æ›´å¥½çš„è·¨å¹³å°å…¼å®¹æ€§
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import scipy.io as sio
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from typing import Tuple, Dict, List, Optional, Union
import warnings
import logging
from pathlib import Path
import pickle
import hashlib
import pandas as pd  # æ·»åŠ è¿™ä¸€è¡Œ

warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class JNUBearingDataset(Dataset):
    """JNUè½´æ‰¿æ•°æ®é›†ç±» - ä¼˜åŒ–ç‰ˆ"""

    def __init__(self,
                 data_path: str,
                 speed: int = 800,
                 shot: int = 5,
                 sample_len: int = 1024,
                 normalize_type: str = 'mean~std',
                 overlap: float = 0.5,
                 mode: str = 'train',
                 fault_types: List[str] = None,
                 lazy_loading: bool = True,
                 cache_data: bool = True):
        """
        åˆå§‹åŒ–JNUè½´æ‰¿æ•°æ®é›†

        Args:
            data_path: æ•°æ®è·¯å¾„
            speed: è½¬é€Ÿ (600, 800, 1000)
            shot: æ¯ç±»æ ·æœ¬æ•°é‡
            sample_len: æ ·æœ¬é•¿åº¦
            normalize_type: æ ‡å‡†åŒ–ç±»å‹ ('mean~std', 'min~max', 'none')
            overlap: æ»‘çª—é‡å ç‡
            mode: æ¨¡å¼ ('train', 'test')
            fault_types: æ•…éšœç±»å‹åˆ—è¡¨
            lazy_loading: æ˜¯å¦ä½¿ç”¨æ‡’åŠ è½½
            cache_data: æ˜¯å¦ç¼“å­˜æ•°æ®
        """
        self.data_path = Path(data_path)
        self.speed = speed
        self.shot = shot
        self.sample_len = sample_len
        self.normalize_type = normalize_type
        self.overlap = overlap
        self.mode = mode
        self.lazy_loading = lazy_loading
        self.cache_data = cache_data

        # é»˜è®¤æ•…éšœç±»å‹
        if fault_types is None:
            self.fault_types = ['Normal', 'InnerRace', 'OuterRace', 'Ball']
        else:
            self.fault_types = fault_types

        self.num_classes = len(self.fault_types)

        # ç±»åˆ«æ˜ å°„
        self.class_to_idx = {fault: idx for idx, fault in enumerate(self.fault_types)}
        self.idx_to_class = {idx: fault for fault, idx in self.class_to_idx.items()}

        # ç¼“å­˜ç›¸å…³
        self.cache_dir = self.data_path / 'cache'
        self.cache_dir.mkdir(exist_ok=True)

        # æ•°æ®å­˜å‚¨
        if self.lazy_loading:
            # æ‡’åŠ è½½æ¨¡å¼ï¼šåªå­˜å‚¨æ–‡ä»¶è·¯å¾„å’Œç´¢å¼•
            self.sample_indices = self._build_sample_indices()
            self.data = None
            self.labels = None
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šåŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
            self.data, self.labels = self._load_data()

        logger.info(f"JNU Dataset initialized:")
        logger.info(f"  Speed: {self.speed} rpm")
        logger.info(f"  Mode: {self.mode}")
        logger.info(f"  Samples per class: {self.shot}")
        logger.info(f"  Total samples: {len(self)}")
        logger.info(f"  Sample length: {self.sample_len}")
        logger.info(f"  Classes: {self.fault_types}")
        logger.info(f"  Lazy loading: {self.lazy_loading}")

    def _build_sample_indices(self) -> List[Dict]:
        """æ„å»ºæ ·æœ¬ç´¢å¼•ï¼ˆç”¨äºæ‡’åŠ è½½ï¼‰"""
        sample_indices = []

        for fault_idx, fault_type in enumerate(self.fault_types):
            file_path = self._get_file_path(fault_type)

            if not file_path.exists():
                logger.warning(f"File not found: {file_path}")
                continue

            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._get_cache_key(file_path, fault_type)
            cached_indices = self._load_cached_indices(cache_key)

            if cached_indices is not None:
                logger.info(f"Loaded cached indices for {fault_type}")
                sample_indices.extend(cached_indices)
            else:
                # ç”Ÿæˆæ–°çš„ç´¢å¼•
                try:
                    raw_data = self._load_raw_file(file_path)
                    if raw_data is not None:
                        indices = self._generate_sample_indices(
                            file_path, raw_data, fault_idx, fault_type
                        )
                        sample_indices.extend(indices)

                        # ç¼“å­˜ç´¢å¼•
                        if self.cache_data:
                            self._save_cached_indices(cache_key, indices)

                except Exception as e:
                    logger.error(f"Failed to process {file_path}: {str(e)}")
                    continue

        return sample_indices

    def _generate_sample_indices(self, file_path: Path, raw_data: np.ndarray,
                                 fault_idx: int, fault_type: str) -> List[Dict]:
        """ç”Ÿæˆæ ·æœ¬ç´¢å¼•"""
        indices = []

        if len(raw_data) < self.sample_len:
            # æ•°æ®é•¿åº¦ä¸è¶³ï¼Œè¿›è¡Œé‡å¤
            repeat_times = self.sample_len // len(raw_data) + 1
            raw_data = np.tile(raw_data, repeat_times)

        # è®¡ç®—æ­¥é•¿
        step = int(self.sample_len * (1 - self.overlap)) if self.overlap > 0 else self.sample_len

        # ç”Ÿæˆç´¢å¼•
        start = 0
        sample_count = 0

        while start + self.sample_len <= len(raw_data) and sample_count < self.shot:
            index_info = {
                'file_path': str(file_path),
                'start_idx': start,
                'end_idx': start + self.sample_len,
                'label': fault_idx,
                'fault_type': fault_type
            }
            indices.append(index_info)
            start += step
            sample_count += 1

        # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œè¿›è¡Œéšæœºé‡‡æ ·è¡¥å……
        while len(indices) < self.shot:
            random_start = np.random.randint(0, max(1, len(raw_data) - self.sample_len))
            index_info = {
                'file_path': str(file_path),
                'start_idx': random_start,
                'end_idx': random_start + self.sample_len,
                'label': fault_idx,
                'fault_type': fault_type
            }
            indices.append(index_info)

        # å¦‚æœæ ·æœ¬å¤ªå¤šï¼Œéšæœºé€‰æ‹©
        if len(indices) > self.shot:
            selected_indices = np.random.choice(len(indices), self.shot, replace=False)
            indices = [indices[i] for i in selected_indices]

        return indices

    def _get_cache_key(self, file_path: Path, fault_type: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ã€é…ç½®å‚æ•°ç”Ÿæˆå”¯ä¸€é”®
        key_string = f"{file_path}_{self.speed}_{self.shot}_{self.sample_len}_{self.overlap}_{fault_type}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def _load_cached_indices(self, cache_key: str) -> Optional[List[Dict]]:
        """åŠ è½½ç¼“å­˜çš„ç´¢å¼•"""
        if not self.cache_data:
            return None

        cache_file = self.cache_dir / f"indices_{cache_key}.pkl"

        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"Failed to load cache {cache_file}: {str(e)}")
                # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                cache_file.unlink(missing_ok=True)

        return None

    def _save_cached_indices(self, cache_key: str, indices: List[Dict]):
        """ä¿å­˜ç´¢å¼•åˆ°ç¼“å­˜"""
        if not self.cache_data:
            return

        cache_file = self.cache_dir / f"indices_{cache_key}.pkl"

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(indices, f)
            logger.debug(f"Cached indices to {cache_file}")
        except Exception as e:
            logger.warning(f"Failed to save cache {cache_file}: {str(e)}")

    def _load_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """åŠ è½½æ•°æ®ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰"""
        all_data = []
        all_labels = []

        for fault_idx, fault_type in enumerate(self.fault_types):
            file_path = self._get_file_path(fault_type)

            if not file_path.exists():
                logger.warning(f"File not found: {file_path}")
                continue

            try:
                fault_data = self._load_raw_file(file_path)
                if fault_data is None:
                    logger.warning(f"Failed to load data from {file_path}")
                    continue

                # ç”Ÿæˆæ ·æœ¬
                samples = self._generate_samples(fault_data)

                # æ ¹æ®shoté™åˆ¶æ ·æœ¬æ•°é‡
                if len(samples) < self.shot:
                    # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œè¿›è¡Œé‡å¤é‡‡æ ·
                    indices = np.random.choice(len(samples), self.shot, replace=True)
                    samples = samples[indices]
                else:
                    # éšæœºé€‰æ‹©shotä¸ªæ ·æœ¬
                    indices = np.random.choice(len(samples), self.shot, replace=False)
                    samples = samples[indices]

                # æ·»åŠ åˆ°æ€»æ•°æ®
                all_data.append(samples)
                all_labels.extend([fault_idx] * len(samples))

            except Exception as e:
                logger.error(f"Error processing {fault_type}: {str(e)}")
                continue

        if not all_data:
            raise ValueError("No valid data found in the dataset")

        # åˆå¹¶æ•°æ®
        data = np.vstack(all_data)
        labels = np.array(all_labels)

        # æ•°æ®æ ‡å‡†åŒ–
        data = self._normalize_data(data)

        return data, labels

    def _get_file_path(self, fault_type: str) -> Path:
        """æ ¹æ®æ•…éšœç±»å‹å’Œè½¬é€Ÿè·å–æ–‡ä»¶è·¯å¾„ - é€‚é…CSVå‘½åæ ¼å¼"""

        # æ–°çš„æ•…éšœç±»å‹æ˜ å°„ï¼ˆé€‚é…ä½ çš„æ–‡ä»¶å‘½åï¼‰
        fault_mapping = {
            'Normal': 'n',  # normal -> n
            'InnerRace': 'ib',  # inner race -> ib
            'OuterRace': 'ob',  # outer race -> ob
            'Ball': 'rb'  # rolling ball -> rb
        }

        fault_prefix = fault_mapping.get(fault_type, fault_type.lower())

        # æ”¯æŒå¤šç§æ–‡ä»¶å‘½åæ ¼å¼
        possible_filenames = []

        if fault_type == 'Normal':
            # æ­£å¸¸çŠ¶æ€æ–‡ä»¶å¯èƒ½æœ‰ä¸åŒåç¼€
            possible_filenames = [
                f"n{self.speed}_3_2.csv",
                f"n{self.speed}_2.csv",
                f"n{self.speed}.csv"
            ]
        else:
            # æ•…éšœçŠ¶æ€æ–‡ä»¶
            possible_filenames = [
                f"{fault_prefix}{self.speed}_2.csv",
                f"{fault_prefix}{self.speed}.csv"
            ]

        # æŸ¥æ‰¾å­˜åœ¨çš„æ–‡ä»¶
        for filename in possible_filenames:
            file_path = self.data_path / filename
            if file_path.exists():
                return file_path

        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤å€¼
        return self.data_path / possible_filenames[0]

        fault_str = fault_mapping.get(fault_type, fault_type.lower())

        # æ”¯æŒå¤šç§æ–‡ä»¶å‘½åæ ¼å¼
        possible_filenames = [
            f"JNU_{fault_str}_{self.speed}rpm.mat",
            f"JNU_{fault_str}_{self.speed}.mat",
            f"{fault_str}_{self.speed}rpm.mat",
            f"{fault_str}_{self.speed}.mat",
            f"{fault_type}_{self.speed}.mat"
        ]

        for filename in possible_filenames:
            file_path = self.data_path / filename
            if file_path.exists():
                return file_path

        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤å€¼
        return self.data_path / possible_filenames[0]

    def _load_raw_file(self, file_path: Path) -> Optional[np.ndarray]:
        """åŠ è½½åŸå§‹æ–‡ä»¶æ•°æ® - æ”¯æŒCSVå’ŒMATæ ¼å¼"""
        try:
            if file_path.suffix.lower() == '.csv':
                # åŠ è½½CSVæ–‡ä»¶
                import pandas as pd

                try:
                    # è¯»å–CSVæ–‡ä»¶
                    data = pd.read_csv(str(file_path), header=None)

                    # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œç›´æ¥ä½¿ç”¨
                    if data.shape[1] == 1:
                        signal_data = data.iloc[:, 0].values
                    else:
                        # å¦‚æœæœ‰å¤šåˆ—ï¼Œé€šå¸¸å–æœ€åä¸€åˆ—ï¼ˆä¿¡å·æ•°æ®ï¼‰
                        signal_data = data.iloc[:, -1].values

                    # ç¡®ä¿æ•°æ®æ˜¯ä¸€ç»´çš„
                    if len(signal_data.shape) > 1:
                        signal_data = signal_data.flatten()

                    # ç§»é™¤å¯èƒ½çš„æ— æ•ˆå€¼
                    signal_data = signal_data[~np.isnan(signal_data)]

                    return signal_data.astype(np.float32)

                except Exception as e:
                    logger.error(f"Error reading CSV {file_path}: {str(e)}")
                    return None

            elif file_path.suffix.lower() == '.mat':
                # ä¿æŒåŸæœ‰çš„MATæ–‡ä»¶åŠ è½½é€»è¾‘
                import scipy.io as sio
                mat_data = sio.loadmat(str(file_path))

                # å°è¯•å¸¸è§çš„å˜é‡å
                possible_keys = ['data', 'X', 'vibration', 'signal', 'DE_time', 'value']

                # é¦–å…ˆå°è¯•è‡ªåŠ¨æ£€æµ‹
                for key in mat_data.keys():
                    if not key.startswith('_'):  # å¿½ç•¥å…ƒæ•°æ®
                        data = mat_data[key]
                        if isinstance(data, np.ndarray) and data.size > 1000:
                            if data.ndim > 1:
                                data = data.flatten()
                            return data.astype(np.float32)

                # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œå°è¯•é¢„å®šä¹‰çš„é”®
                for key in possible_keys:
                    if key in mat_data:
                        data = mat_data[key]
                        if data.ndim > 1:
                            data = data.flatten()
                        return data.astype(np.float32)

            logger.warning(f"Unsupported file format: {file_path}")
            return None

        except Exception as e:
            logger.error(f"Error loading {file_path}: {str(e)}")
            return None

            # å°è¯•å¸¸è§çš„å˜é‡å
            possible_keys = ['data', 'X', 'vibration', 'signal', 'DE_time', 'value']

            # é¦–å…ˆå°è¯•è‡ªåŠ¨æ£€æµ‹
            for key in mat_data.keys():
                if not key.startswith('_'):  # å¿½ç•¥å…ƒæ•°æ®
                    data = mat_data[key]
                    if isinstance(data, np.ndarray) and data.size > 1000:
                        # ç¡®ä¿æ˜¯ä¸€ç»´æ•°æ®
                        if data.ndim > 1:
                            data = data.flatten()
                        return data.astype(np.float32)

            # å¦‚æœè‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œå°è¯•é¢„å®šä¹‰çš„é”®
            for key in possible_keys:
                if key in mat_data:
                    data = mat_data[key]
                    if data.ndim > 1:
                        data = data.flatten()
                    return data.astype(np.float32)

            logger.warning(f"No suitable data found in {file_path}")
            return None

        except Exception as e:
            logger.error(f"Error loading {file_path}: {str(e)}")
            return None

    def _generate_samples(self, raw_data: np.ndarray) -> np.ndarray:
        """ä»åŸå§‹æ•°æ®ç”Ÿæˆå›ºå®šé•¿åº¦æ ·æœ¬"""
        if len(raw_data) < self.sample_len:
            # å¦‚æœæ•°æ®é•¿åº¦ä¸è¶³ï¼Œè¿›è¡Œé‡å¤
            repeat_times = self.sample_len // len(raw_data) + 1
            raw_data = np.tile(raw_data, repeat_times)

        # è®¡ç®—æ­¥é•¿
        step = int(self.sample_len * (1 - self.overlap)) if self.overlap > 0 else self.sample_len

        # ç”Ÿæˆæ ·æœ¬
        samples = []
        start = 0

        while start + self.sample_len <= len(raw_data):
            sample = raw_data[start:start + self.sample_len]
            samples.append(sample)
            start += step

            # é™åˆ¶æ ·æœ¬æ•°é‡ï¼Œé¿å…å†…å­˜é—®é¢˜
            if len(samples) >= 1000:
                break

        if len(samples) == 0:
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬ï¼Œç›´æ¥æˆªå–
            samples = [raw_data[:self.sample_len]]

        return np.array(samples, dtype=np.float32)

    def _normalize_data(self, data: np.ndarray) -> np.ndarray:
        """æ•°æ®æ ‡å‡†åŒ–"""
        if self.normalize_type == 'none':
            return data

        original_shape = data.shape
        data_2d = data.reshape(-1, self.sample_len)

        if self.normalize_type == 'mean~std':
            # é€æ ·æœ¬æ ‡å‡†åŒ–
            normalized_data = np.zeros_like(data_2d, dtype=np.float32)
            for i in range(data_2d.shape[0]):
                sample = data_2d[i]
                mean_val = np.mean(sample)
                std_val = np.std(sample)
                if std_val > 1e-8:
                    normalized_data[i] = (sample - mean_val) / std_val
                else:
                    normalized_data[i] = sample - mean_val

        elif self.normalize_type == 'min~max':
            # é€æ ·æœ¬min-maxæ ‡å‡†åŒ–
            normalized_data = np.zeros_like(data_2d, dtype=np.float32)
            for i in range(data_2d.shape[0]):
                sample = data_2d[i]
                min_val = np.min(sample)
                max_val = np.max(sample)
                if max_val > min_val:
                    normalized_data[i] = (sample - min_val) / (max_val - min_val)
                else:
                    normalized_data[i] = sample

        elif self.normalize_type == 'global_std':
            # å…¨å±€æ ‡å‡†åŒ–
            scaler = StandardScaler()
            normalized_data = scaler.fit_transform(data_2d).astype(np.float32)

        else:
            normalized_data = data_2d.astype(np.float32)

        return normalized_data.reshape(original_shape)

    def __len__(self) -> int:
        if self.lazy_loading:
            return len(self.sample_indices)
        else:
            return len(self.data)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        if self.lazy_loading:
            return self._get_lazy_item(idx)
        else:
            return self._get_cached_item(idx)

    def _get_lazy_item(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ‡’åŠ è½½æ¨¡å¼è·å–æ ·æœ¬"""
        index_info = self.sample_indices[idx]

        # åŠ è½½æ•°æ®ç‰‡æ®µ
        try:
            raw_data = self._load_raw_file(Path(index_info['file_path']))
            if raw_data is None:
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¿”å›é›¶å¼ é‡
                sample = np.zeros(self.sample_len, dtype=np.float32)
            else:
                start_idx = index_info['start_idx']
                end_idx = index_info['end_idx']
                sample = raw_data[start_idx:end_idx]

                # æ ‡å‡†åŒ–
                if self.normalize_type == 'mean~std':
                    mean_val = np.mean(sample)
                    std_val = np.std(sample)
                    if std_val > 1e-8:
                        sample = (sample - mean_val) / std_val
                    else:
                        sample = sample - mean_val
                elif self.normalize_type == 'min~max':
                    min_val = np.min(sample)
                    max_val = np.max(sample)
                    if max_val > min_val:
                        sample = (sample - min_val) / (max_val - min_val)
                    else:
                        sample = sample

        except Exception as e:
            logger.warning(f"Error loading sample {idx}: {str(e)}")
            sample = np.zeros(self.sample_len, dtype=np.float32)

        # è½¬æ¢ä¸ºå¼ é‡
        sample_tensor = torch.FloatTensor(sample).unsqueeze(0)  # [1, sample_len]
        label_tensor = torch.LongTensor([index_info['label']])[0]  # æ ‡é‡

        return sample_tensor, label_tensor

    def _get_cached_item(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç¼“å­˜æ¨¡å¼è·å–æ ·æœ¬"""
        sample = self.data[idx]
        label = self.labels[idx]

        # è½¬æ¢ä¸ºå¼ é‡å¹¶æ·»åŠ é€šé“ç»´åº¦
        sample_tensor = torch.FloatTensor(sample).unsqueeze(0)  # [1, sample_len]
        label_tensor = torch.LongTensor([label])[0]  # æ ‡é‡

        return sample_tensor, label_tensor

    def get_class_distribution(self) -> Dict[str, int]:
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        distribution = {}

        if self.lazy_loading:
            for fault_type in self.fault_types:
                count = sum(1 for info in self.sample_indices if info['fault_type'] == fault_type)
                distribution[fault_type] = count
        else:
            for i, fault_type in enumerate(self.fault_types):
                count = np.sum(self.labels == i)
                distribution[fault_type] = count

        return distribution

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        import shutil
        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(exist_ok=True)
            logger.info("Dataset cache cleared")


def create_jnu_dataloaders(
        data_path: str,
        source_speed: int = 800,
        target_speed: int = 1000,
        source_shot: int = 50,
        target_shot: int = 5,
        sample_len: int = 1024,
        batch_size: int = 64,
        normalize_type: str = 'mean~std',
        num_workers: int = 4,
        fault_types: List[str] = None,
        lazy_loading: bool = True
) -> Dict[str, DataLoader]:
    """
    åˆ›å»ºJNUæ•°æ®é›†çš„æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºè·¨åŸŸå®éªŒï¼‰
    """
    # æ£€æŸ¥æ•°æ®è·¯å¾„
    data_path = Path(data_path)
    if not data_path.exists():
        raise FileNotFoundError(f"Data path not found: {data_path}")

    # åˆ›å»ºæºåŸŸè®­ç»ƒé›†
    source_train_dataset = JNUBearingDataset(
        data_path=str(data_path),
        speed=source_speed,
        shot=source_shot,
        sample_len=sample_len,
        normalize_type=normalize_type,
        mode='train',
        fault_types=fault_types,
        lazy_loading=lazy_loading
    )

    # åˆ›å»ºæºåŸŸæµ‹è¯•é›†
    source_test_dataset = JNUBearingDataset(
        data_path=str(data_path),
        speed=source_speed,
        shot=target_shot,
        sample_len=sample_len,
        normalize_type=normalize_type,
        mode='test',
        fault_types=fault_types,
        lazy_loading=lazy_loading
    )

    # åˆ›å»ºç›®æ ‡åŸŸæ•°æ®é›†
    target_dataset = JNUBearingDataset(
        data_path=str(data_path),
        speed=target_speed,
        shot=target_shot,
        sample_len=sample_len,
        normalize_type=normalize_type,
        mode='test',
        fault_types=fault_types,
        lazy_loading=lazy_loading
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloaders = {
        'source_train': DataLoader(
            source_train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available(),
            drop_last=True,
            persistent_workers=num_workers > 0
        ),
        'source_test': DataLoader(
            source_test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available(),
            drop_last=False,
            persistent_workers=num_workers > 0
        ),
        'target': DataLoader(
            target_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available(),
            drop_last=False,
            persistent_workers=num_workers > 0
        )
    }

    logger.info(f"Created dataloaders:")
    logger.info(f"  Source train: {len(source_train_dataset)} samples")
    logger.info(f"  Source test: {len(source_test_dataset)} samples")
    logger.info(f"  Target: {len(target_dataset)} samples")

    return dataloaders


def create_single_domain_dataloader(
        data_path: str,
        speed: int = 800,
        train_shot: int = 40,
        test_shot: int = 10,
        sample_len: int = 1024,
        batch_size: int = 64,
        normalize_type: str = 'mean~std',
        num_workers: int = 4,
        fault_types: List[str] = None,
        lazy_loading: bool = True
) -> Dict[str, DataLoader]:
    """
    åˆ›å»ºå•åŸŸæ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºåŸºç¡€å®éªŒï¼‰
    """
    # åˆ›å»ºè®­ç»ƒé›†
    train_dataset = JNUBearingDataset(
        data_path=data_path,
        speed=speed,
        shot=train_shot,
        sample_len=sample_len,
        normalize_type=normalize_type,
        mode='train',
        fault_types=fault_types,
        lazy_loading=lazy_loading
    )

    # åˆ›å»ºæµ‹è¯•é›†
    test_dataset = JNUBearingDataset(
        data_path=data_path,
        speed=speed,
        shot=test_shot,
        sample_len=sample_len,
        normalize_type=normalize_type,
        mode='test',
        fault_types=fault_types,
        lazy_loading=lazy_loading
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloaders = {
        'train': DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available(),
            drop_last=True,
            persistent_workers=num_workers > 0
        ),
        'test': DataLoader(
            test_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available(),
            drop_last=False,
            persistent_workers=num_workers > 0
        )
    }

    return dataloaders


# æ•°æ®é›†ä¿¡æ¯è·å–å‡½æ•°
def get_dataset_info(data_path: str) -> Dict:
    """è·å–æ•°æ®é›†ä¿¡æ¯"""
    data_path = Path(data_path)

    # æ‰«æå¯ç”¨æ–‡ä»¶
    available_files = []
    if data_path.exists():
        available_files = list(data_path.glob("*.mat"))

    info = {
        'dataset_name': 'JNU Bearing Dataset',
        'fault_types': ['Normal', 'InnerRace', 'OuterRace', 'Ball'],
        'available_speeds': [600, 800, 1000],
        'data_path': str(data_path),
        'available_files': [f.name for f in available_files],
        'file_format': '.mat files',
        'recommended_settings': {
            'sample_len': 1024,
            'normalize_type': 'mean~std',
            'overlap': 0.5,
            'batch_size': 64,
            'lazy_loading': True
        }
    }
    return info


# æµ‹è¯•å‡½æ•°
def test_jnu_dataset():
    """æµ‹è¯•JNUæ•°æ®é›†åŠ è½½"""
    print("Testing optimized JNU Dataset...")

    # ä½¿ç”¨å½“å‰ç›®å½•ä½œä¸ºæµ‹è¯•è·¯å¾„
    data_path = "./data/JNU/"

    try:
        # æµ‹è¯•æ•°æ®é›†ä¿¡æ¯
        dataset_info = get_dataset_info(data_path)
        print("âœ“ Dataset info retrieved")
        print(f"  Available files: {len(dataset_info['available_files'])}")

        # å¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼Œåˆ›å»ºè™šæ‹Ÿæµ‹è¯•
        if not dataset_info['available_files']:
            print("  No real data found, using dummy dataset for testing")

            # åˆ›å»ºè™šæ‹Ÿæ•°æ®é›†æµ‹è¯•
            from datasets.data_utils import BaseDataset

            dummy_data = np.random.randn(20, 1024).astype(np.float32)
            dummy_labels = np.random.randint(0, 4, 20)

            dummy_dataset = BaseDataset(dummy_data, dummy_labels)
            print(f"âœ“ Dummy dataset created: {len(dummy_dataset)} samples")

            # æµ‹è¯•æ•°æ®åŠ è½½
            sample, label = dummy_dataset[0]
            print(f"âœ“ Sample loading: {sample.shape}, label: {label}")

        else:
            print("  Real data found, testing with actual files")

            # æµ‹è¯•çœŸå®æ•°æ®é›†
            try:
                dataset = JNUBearingDataset(
                    data_path=data_path,
                    speed=800,
                    shot=5,
                    sample_len=1024,
                    lazy_loading=True
                )

                print(f"âœ“ Real dataset created: {len(dataset)} samples")

                # æµ‹è¯•æ•°æ®åŠ è½½
                sample, label = dataset[0]
                print(f"âœ“ Real sample loading: {sample.shape}, label: {label}")

                # æµ‹è¯•ç±»åˆ«åˆ†å¸ƒ
                distribution = dataset.get_class_distribution()
                print(f"âœ“ Class distribution: {distribution}")

            except Exception as e:
                print(f"âœ— Real dataset test failed: {str(e)}")
                print("  This might be due to file format or path issues")

        return True

    except Exception as e:
        print(f"âœ— Dataset test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    # è¿è¡Œæµ‹è¯•
    success = test_jnu_dataset()

    if success:
        print("\nğŸ‰ Dataset module test completed successfully!")
    else:
        print("\nâŒ Dataset module test failed!")

    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    try:
        dataset_info = get_dataset_info("./data/JNU/")
        print("\nDataset Info:")
        for key, value in dataset_info.items():
            if key != 'available_files' or len(value) < 10:
                print(f"  {key}: {value}")
            else:
                print(f"  {key}: {len(value)} files")
    except Exception as e:
        print(f"Failed to get dataset info: {e}")